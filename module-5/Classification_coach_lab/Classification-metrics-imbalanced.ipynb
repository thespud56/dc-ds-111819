{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating and Tuning a Binary Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "After this lesson, you should be able to:\n",
    "\n",
    "- Build and explain confusion matrices from a model output\n",
    "- Calculate various binary classification metrics\n",
    "- Explain the AUC/ROC curve, why it matters, and how to use it\n",
    "- Understand when and how to optimize a model for various metrics\n",
    "- Optimize a classification model based on costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category definitions - possible outcomes in binary classification\n",
    " \n",
    "#### - TP = True Positive (class 1 correctly classified as class 1) - e.g. Patient with cancer tests positive for cancer\n",
    "#### - TN = True Negative (class 0 correctly classified as class 0) - e.g. Patient without cancer tests negative for cancer\n",
    "#### - FP = False Positive (class 0 incorrectly classified as class 1) - e.g. Patient without cancer tests positive for cancer\n",
    "#### - FN = False Negative (class 1 incorrectly classified as class 0) - e.g. Patient with cancer tests negative for cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ \\text{Possible misclassifications} $$\n",
    "\n",
    "![Type 1 vs. Type 2 Error](images/type-1-type-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run a model and look at some metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target', axis = 1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 77, stratify = y, test_size = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 77)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Default Measure (in most prebuilt models) - Accuracy\n",
    "\n",
    "$$ \\frac{(TP + TN)}{(TP + FP + TN + FN)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We got an accuracy score of .842, but what does that tell us? Just that 84.2% of the time we are correct, nothing about how we are correct or how we are wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(X_test)\n",
    "actual = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(actual, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My eyes!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(actual, predictions), columns = ['predicted 0', 'predicted 1'], \n",
    "             index = ['actual 0', 'actual 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We got more false negatives than false positives. What would we likely prefer in the case of this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other metrics\n",
    "\n",
    "### Misclassification Rate\n",
    "#### $$ 1 - \\text{accuracy} $$ \n",
    "\n",
    "### $$ {OR} $$\n",
    "\n",
    "#### $$ \\frac{FP + FN}{TP + FP + TN + FN} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity (AKA True Positive Rate, Recall, and Probability of Detection)\n",
    "\n",
    "$$ \\frac{TP}{TP + FN} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity (AKA True Negative Rate)\n",
    "\n",
    "$$ \\frac{TN}{TN + FP} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision (AKA Positive Predictive Value)\n",
    "\n",
    "$$ \\frac{TP}{TP + FP} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate\n",
    "\n",
    "$$ \\frac{FP}{FP + TN} $$\n",
    "\n",
    "OR\n",
    "\n",
    "#### 1 - Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Predictive Value\n",
    "\n",
    "$$ \\frac{TN}{TN + FN} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "### $$ 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$\n",
    "\n",
    "#### Useful with imbalanced classes where the Negative class is the majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Accuracy\n",
    "\n",
    "### $$ \\frac{\\text{Sensitivity + Specificity}}{2} $$\n",
    "\n",
    "#### Useful with imbalanced classes where the Positive class is the majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the Binary Classification Metrics\n",
    "\n",
    "![classification metrics](./images/conf_matrix_classification_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which of these metrics would we want to optimize for in a heart disease detection algorithm?\n",
    "\n",
    "False Positives and False Negatives each have some cost associated with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's figure out how to optimize!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember that Random Forest gives probability predictions for each class, in addition to the final classification. By default, a majority of trees voting for a class determines the classification, but we can adjust that threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "for item in rf.predict_proba(X_test):\n",
    "    if item[0] <= .49:\n",
    "        predicts.append(1)\n",
    "    else:\n",
    "        predicts.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_test, predicts), index = ['actual 0', 'actual 1'], \n",
    "             columns = ['predicted 0', 'predicted 1'])\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy\n",
    "\n",
    "(conf_matrix['predicted 0'][0] + conf_matrix['predicted 1'][1]) / len(predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The AUC / ROC curve (Area Under Curve of the Receiver Operating Characteristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "\n",
    "for x in np.linspace(0, 1, 100):\n",
    "    \n",
    "    # Same predictions based on predict_proba thresholds\n",
    "    predicts = []\n",
    "    \n",
    "    for item in rf.predict_proba(X_test):\n",
    "        if item[0] <= x:\n",
    "            predicts.append(1)\n",
    "        else:\n",
    "            predicts.append(0)\n",
    "\n",
    "    conf_matrix = pd.DataFrame(confusion_matrix(y_test, predicts), index = ['actual 0', 'actual 1'], \n",
    "                     columns = ['predicted 0', 'predicted 1'])\n",
    "    \n",
    "    \n",
    "    # Assign TP, TN, FP, FN\n",
    "    true_positives = conf_matrix['predicted 1'][1]\n",
    "    true_negatives = conf_matrix['predicted 0'][0]\n",
    "    false_positives = conf_matrix['predicted 1'][0]\n",
    "    false_negatives = conf_matrix['predicted 0'][1]\n",
    "\n",
    "    \n",
    "    # Calculate Sensitivity and Specificity\n",
    "    sensitivity = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    specificity = true_negatives / (true_negatives + false_positives)\n",
    "    \n",
    "    \n",
    "    # Append to lists to graph\n",
    "    x_list.append(1 - specificity)\n",
    "\n",
    "    y_list.append(sensitivity)\n",
    "\n",
    "    \n",
    "# Plot ROC curve\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.title('ROC Curve', fontsize = 20)\n",
    "plt.xlabel('1 - specificity (False Positive Rate)', fontsize = 15)\n",
    "plt.ylabel('sensitivity (True Positive Rate)', fontsize = 15)\n",
    "plt.xlim(-0.01, 1)\n",
    "plt.ylim(-0.01, 1)\n",
    "plt.plot(x_list, y_list);\n",
    "plt.plot([0, 1], [0, 1]);\n",
    "\n",
    "# x = 1 - specificity\n",
    "# y = sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(x_list, y_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add associated costs to our TP, FP, TN, FN to our loop and minimize the cost\n",
    "This is the naive way to optimize, but works well - you could also create a closed form optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(model, X_test, y_test, num_thres = 100, cost_fp = 3, cost_tn = 0.5, cost_tp = 1, cost_fn = 2):\n",
    "\n",
    "    _thres = []; tpr = [] ; fpr = [] ; cost = []\n",
    "\n",
    "    # assign model predictions\n",
    "    prediction = model.predict_proba(X_test)\n",
    "\n",
    "    ## Different code for same objective to calculate metrics at thresholds\n",
    "    \n",
    "    for thres in np.linspace(0.01, 1, num_thres):\n",
    "        \n",
    "        _thres.append(thres)\n",
    "        predicts = np.zeros((prediction.shape[0], 1)) \n",
    "        predicts[np.where(prediction[:, 1] >= thres)] = 1\n",
    "\n",
    "        conf_matrix = confusion_matrix(y_test, predicts)\n",
    "\n",
    "        tp = conf_matrix[1, 1]\n",
    "        tn = conf_matrix[0, 0]\n",
    "        fp = conf_matrix[0, 1]\n",
    "        fn = conf_matrix[1, 0]\n",
    "\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        tnr = specificity = tn / (tn + fp)\n",
    "        fnr = 1 - sensitivity\n",
    "\n",
    "        tpr.append(sensitivity)\n",
    "    \n",
    "        fpr.append(1 - specificity)\n",
    "        \n",
    "        # add a cost function (this involves domain knowledge)\n",
    "        \n",
    "        current_cost = (cost_fp * fp) + (cost_tn * tn) + (cost_tp * tp) + (cost_fn * fn)\n",
    "            \n",
    "        cost.append(current_cost)  \n",
    "\n",
    "    return fpr, tpr, cost, _thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, cost, thres = cost_function(model = rf, X_test = X_test, y_test = y_test,\n",
    "                                      num_thres = 100, cost_fp = 2, cost_tn = 1, cost_tp = 1, cost_fn = 3)\n",
    "\n",
    "cost_idx = np.argmin(cost)\n",
    "min_cost_threshold = fpr[cost_idx], tpr[cost_idx], thres[cost_idx]\n",
    "\n",
    "ax = plt.figure(figsize = (10, 8))\n",
    "plt.title('ROC Curve', fontsize = 20)\n",
    "plt.xlabel('1 - specificity', fontsize = 15)\n",
    "plt.ylabel('sensitivity', fontsize = 15)\n",
    "plt.xlim(-.01, 1.01)\n",
    "plt.ylim(-.01, 1.01)\n",
    "plt.plot(fpr, tpr);\n",
    "plt.plot([0, 1], [0, 1]);\n",
    "plt.scatter(min_cost_threshold[0], min_cost_threshold[1], marker ='o', color = 'red', s=250)\n",
    "ax.text(min_cost_threshold[0] + 0.06, min_cost_threshold[1] - 0.03, 'Threshold:'+ str(round(min_cost_threshold[2], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing costs on multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_class = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_multi_model(model_list, X_test, y_test, num_thres = 100, cost_fp = 3, \n",
    "                              cost_tn = 0.5, cost_tp = 1, cost_fn = 2):\n",
    "    '''model_list expects a list of already fit models - You could add the model.fit() code to the function.\n",
    "    models in model_list MUST have the predict_proba() method - this could be modified in the future'''\n",
    "    \n",
    "    best_cost = []\n",
    "    best_thresh = []\n",
    "    \n",
    "    for model in model_list:\n",
    "        \n",
    "        _thres = []\n",
    "        cost = []\n",
    "\n",
    "    # assign model predictions\n",
    "        prediction = model.predict_proba(X_test)\n",
    "\n",
    "    ## Different code for same objective to calculate metrics at thresholds\n",
    "    \n",
    "        for thres in np.linspace(0.01, 1, num_thres):\n",
    "\n",
    "            _thres.append(thres)\n",
    "            predicts = np.zeros((prediction.shape[0], 1)) \n",
    "            predicts[np.where(prediction[:, 1] >= thres)] = 1\n",
    "\n",
    "            conf_matrix = confusion_matrix(y_test, predicts)\n",
    "\n",
    "            tp = conf_matrix[1, 1]\n",
    "            tn = conf_matrix[0, 0]\n",
    "            fp = conf_matrix[0, 1]\n",
    "            fn = conf_matrix[1, 0]\n",
    "\n",
    "            # add a cost function (this involves domain knowledge)\n",
    "\n",
    "            current_cost = (cost_fp * fp) + (cost_tn * tn) + (cost_tp * tp) + (cost_fn * fn)\n",
    "\n",
    "            cost.append(current_cost)\n",
    "        \n",
    "        thresh_idx = np.array(cost).argmin()\n",
    "        best_cost.append(min(cost))\n",
    "        best_thresh.append(_thres[thresh_idx])\n",
    "\n",
    "    return pd.DataFrame({'model':model_list, 'best_cost' : best_cost, 'best_thresh' : best_thresh})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function_multi_model([rf, logreg, nb_class], X_test = X_test, y_test = y_test,\n",
    "                         num_thres = 100, cost_fp = 3, cost_tn = 0.5, cost_tp = 1, cost_fn = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Threshold vs. Population Distribution\n",
    "\n",
    "[ROC Curve Interactive Visualizer](http://www.navan.name/roc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict_proba(X_test)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict(X_test)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cancer_dist = []\n",
    "cancer_dist = []\n",
    "\n",
    "for item in rf.predict_proba(X_test):\n",
    "    if item[0] <= .49:\n",
    "        cancer_dist.append(item[0])\n",
    "    else:\n",
    "        no_cancer_dist.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.title('Distributions of Population of Patients with and without Cancer')\n",
    "plt.xlabel('Threshold', fontsize = 14)\n",
    "\n",
    "sns.distplot(no_cancer_dist, bins = 15, color = 'red')\n",
    "sns.distplot(cancer_dist, bins = 15, color = 'blue')\n",
    "plt.legend(['no cancer dist', 'cancer dist']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve vs Population Separation\n",
    "![a](images/pop-curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![d](images/varying_dist_roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = pd.read_csv('./data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fraud_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df['Class'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fraud_df[fraud_df['Amount'] == 0]['Class'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (12,8))\n",
    "# sns.heatmap(fraud_df.corr()[['Class']].sort_values(by = 'Class'), cmap = 'coolwarm', \n",
    "#             vmin = -1, vmax = 1, annot = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fraud_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fraud_df.loc[:, 'V26' : 'V27']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fraud_df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 1 - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fraud_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test, fraud_rf.predict(X_test)), columns = ['predicted 0', 'predicted 1'], \n",
    "             index = ['actual 0', 'actual 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roc_curve(y_test, fraud_rf.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, fraud_rf.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.title('ROC Curve', fontsize = 20)\n",
    "plt.xlabel('1 - specificity', fontsize = 15)\n",
    "plt.ylabel('sensitivity', fontsize = 15)\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.plot(fpr,tpr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshs = precision_recall_curve(y_test, fraud_rf.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.xlabel('Recall (TP / (TP + FN))', fontsize = 18)\n",
    "plt.ylabel('Precision (TP / (TP + FP))', fontsize = 18)\n",
    "plt.plot([0,1], [0,0])\n",
    "plt.plot(recall, precision);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fraud_df.drop(columns=['Class', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fraud_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fraud = RandomForestClassifier(n_estimators = 100, n_jobs = -1, oob_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fraud.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fraud.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fraud.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_preds = rf_fraud.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test, fraud_preds), columns = ['predicted 0', 'predicted 1'], \n",
    "             index = ['actual 0', 'actual 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fraud.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_curve(y_test, rf_fraud.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, rf_fraud.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.title('ROC Curve', fontsize = 20)\n",
    "plt.xlabel('1 - specificity', fontsize = 15)\n",
    "plt.ylabel('sensitivity', fontsize = 15)\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.plot(fpr,tpr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshs = precision_recall_curve(y_test, rf_fraud.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.xlabel('Recall (TP / (TP + FN))', fontsize = 18)\n",
    "plt.ylabel('Precision (TP / (TP + FP))', fontsize = 18)\n",
    "plt.plot([0,1], [0,0])\n",
    "plt.plot(recall, precision);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try our cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, cost, thres = cost_function(model = rf_fraud, X_test = X_test, y_test = y_test,\n",
    "                                      num_thres = 100, cost_fp = 1, cost_tn = 1, cost_tp = 1, cost_fn = 5)\n",
    "\n",
    "cost_idx = np.argmin(cost)\n",
    "min_cost_threshold = fpr[cost_idx], tpr[cost_idx], thres[cost_idx]\n",
    "\n",
    "ax = plt.figure(figsize = (10, 8))\n",
    "plt.title('ROC Curve', fontsize = 20)\n",
    "plt.xlabel('1 - specificity', fontsize = 15)\n",
    "plt.ylabel('sensitivity', fontsize = 15)\n",
    "plt.xlim(-.01, 1.01)\n",
    "plt.ylim(-.01, 1.01)\n",
    "plt.plot(fpr, tpr);\n",
    "plt.plot([0, 1], [0, 1]);\n",
    "plt.scatter(min_cost_threshold[0], min_cost_threshold[1], marker ='o', color = 'red', s=250)\n",
    "ax.text(min_cost_threshold[0] + 0.06, min_cost_threshold[1] - 0.03, 'Threshold:'+ str(round(min_cost_threshold[2], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(k_neighbors=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_smote = RandomForestClassifier(n_estimators=100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_smote.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_smote.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_preds = rf_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test, fraud_preds), columns = ['predicted 0', 'predicted 1'], \n",
    "             index = ['actual 0', 'actual 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, rf_smote.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.title('ROC Curve', fontsize = 20)\n",
    "plt.xlabel('1 - specificity', fontsize = 15)\n",
    "plt.ylabel('sensitivity', fontsize = 15)\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.plot(fpr,tpr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
